---
title: "Exam 1"
author: "Jacob Harrell"
date: "2/17/2021"
output: html_document
---

Question 1: Load Data into R and inspect first several rows.

```{r}

setwd(choose.dir('ExamData'))
data<-read.csv(file='Exam 1 Data.csv')

head(data)

```

Question 2: Fit linear model. Response is a function of X1, X2, and X3.
Interaction is between X1 and X2 only.

```{r}

setwd(choose.dir('ExamData'))
data<-read.csv(file='Exam 1 Data.csv')

fit<-lm(formula= y~x1*x2+x3, data = data)
summary(fit)

```

Question 3: Interpret the effect of variable x1 when x2 is -1.

First, the general formula for the linear model:
y=b0+b1*x1+b2*x2+b3*x3+b4*x3+b5*x1*x2

And, somewhat simplified:
y=b0+x1(b1+b5*x2)+b2*x2+b3*x3+b4*x3

```{r}

setwd(choose.dir('ExamData'))
data<-read.csv(file='Exam 1 Data.csv')

fit<-lm(formula= y~x1*x2+x3, data = data)

B<-coef(fit)
B[2]+B[6]-1

```

Question 4: Interpret the effect of variable x1 when x2 is 1.

```{r}

setwd(choose.dir('ExamData'))
data<-read.csv(file='Exam 1 Data.csv')

fit<-lm(formula= y~x1*x2+x3, data = data)

B<-coef(fit)
B[2]+B[6]+1

```


Question 5: Interpret the effect of variable x3

```{r}

setwd(choose.dir('ExamData'))
data<-read.csv(file='Exam 1 Data.csv')

fit<-lm(formula= y~x1*x2+x3, data = data)

B<-coef(fit)

data$b<-ifelse(data$x3=='b',1,0)
data$b
B[4]*data$b

data$c<-ifelse(data$x3=='c',1,0)
data$c
B[5]*data$c

```

When all other variables are constant, difference between a and b is -1.627162.
When all other variables are constant, difference between a and b is 0.002504032.

Question 6: Describe how R codes the categorical variable x3. Demonstrate by reporting the first 5 values of variables derived from x3.

R codes the categorical x3 by first creating dummy variables.
The number of dummy variables created is k-1, where k equals the number of variables.
In the case of this exam data, two dummy variables are created.
Variable 'a' is used as a reference.
Variables 'b' and 'c' are used as dummy variables with values 0 and 1.
These are binomial, where 0 indicates absence of a variable and 1 indicates presence.

```{r}

setwd(choose.dir('ExamData'))
data<-read.csv(file='Exam 1 Data.csv')

cbind(data$x3[1:5],
      ifelse(data$x3=='b', 1, 0)[1:5],
      ifelse(data$x3=='c', 1, 0)[1:5])

```
Column 2 indicates the presense or absense of variable 'b'.
Column 3 indicates the presense or absense of variable 'c'.

Question 7: Derive the test statistic and p-value associated with the interaction between x1 and x2. 
What is the null hypotheses assumed by the lm() function?
Do we reject or fail to reject the null hypotheses?

```{r}

setwd(choose.dir('ExamData'))
data<-read.csv(file='Exam 1 Data.csv')

fit<-lm(formula= y~x1*x2+x3, data = data)

ts<-coef(fit)[6]/summary(fit)[['coefficients']]['x1:x2', 'Std. Error']
ts

pt(q=ts, df=nrow(data)-length(coef(fit)))*2
summary(fit)[['coefficients']]['x1:x2','Pr(>|t|)']

```

the test statistic for x1:x2 is 1.664819. The p-value for x1:x2 is .099279.
The null hypothesis assumed by the lm() function is that the coefficient associated with the respective variable at 'x1:x2' is equal to zero, meaning that statistically there is no change within this varible. Because the respective p-value is .09927881 which is within the range of what would be a reasonable (a), in this particular instance we would fail to reject the null hypothesis. 

Question 8:Assume you have the following realizations of random variable Y :
y = (3, 8, 7)
Further assume realizations of the random variable Y are Gaussian distributed:
y ∼ Gaussian(µ, σ2
).
Fix σ2 = 1 and µ = 8, and evaluate the probability density at each of your 3 realizations

```{r}

mu<-8
v<-1
y1<-3
y2<-8
y3<-7

dens1<-1/(sqrt(2*pi*v))*exp(-(y1-mu)^2/(2*v))
dens1

dens2<-1/(sqrt(2*pi*v))*exp(-(y2-mu)^2/(2*v))
dens1

dens3<-1/(sqrt(2*pi*v))*exp(-(y3-mu)^2/(2*v))
dens3

```


Question 9: What is a type I error? What is a p-value? How are the two quantities related?

A type I error is when we falsely reject a null hypothesis that happens to be true. 
The p-value is the probability of observing an extreme test statistic, based on whatever the null hypothesis happens to be.  

Statistically speaking, a type 1 error can occur when an observation of a test statistic is in the extreme even though the null hypothesis is true. In this case, the p-value can be used to reduce the likelihood of a type 1 error. Generally, the p-value is associated with the probability of a type I error. A p-value of .01 would correspond with a type 1 error probability of .01. A p-value of .001 would correspond with a type 1 error of .001. The smaller the p-value, the lower the probability of observing a type 1 error.

Question 10: What is the fundamental assumption we must make to derive inference about regression coefficients of a linear model?

The fundamental assumption is that regression coefficients are uniformaly distributed across the defined linear model where the change in y is associated with a 1-unit change in x.

